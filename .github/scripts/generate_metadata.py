#!/usr/bin/env python3
"""
TrueNAS Apps Capability Manager

This script analyzes Docker Compose configurations for TrueNAS apps and updates
their metadata with capability requirements extracted from rendered templates.
"""

import os
import sys
import yaml
import logging
import argparse
import subprocess
from pathlib import Path
from typing import Dict, List, Set, Optional
from dataclasses import dataclass


# Global configuration
class Config:
    CONTAINER_IMAGE = "ghcr.io/truenas/apps_validation:latest"
    PLATFORM = "linux/amd64"

    # Directory structure
    APPS_ROOT_DIR = "ix-dev"
    TEST_VALUES_DIR = "templates/test_values"
    RENDERED_COMPOSE_PATH = "templates/rendered/docker-compose.yaml"

    # File names
    APP_METADATA_FILE = "app.yaml"
    APP_VALUES_FILE = "ix_values.yaml"
    QUESTIONS_FILE = "questions.yaml"

    # Special apps excluded from test train
    EXCLUDED_TEST_APPS = {"other-nginx", "nginx"}


# Setup logging
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class DockerCapability:
    """Represents a Docker capability with its human-readable description."""

    name: str
    description: str

    def to_dict(self) -> Dict[str, str]:
        return {"description": self.description, "name": self.name}


@dataclass
class AppAnalysisResult:
    """Results from analyzing an app's Docker compose configurations."""

    capabilities: List[DockerCapability]
    service_names: List[str]
    app_version: str


@dataclass
class AppManifest:
    """Represents an app's basic information and test configurations."""

    path: Path
    name: str
    train: str
    test_value_files: List[str]


class DockerCapabilityRegistry:
    """Registry of Docker capabilities with their descriptions."""

    _CAPABILITY_DESCRIPTIONS = {
        "AUDIT_CONTROL": "able to control audit subsystem configuration",
        "AUDIT_READ": "able to read audit log entries",
        "AUDIT_WRITE": "able to write records to audit log",
        "BLOCK_SUSPEND": "able to block system suspend operations",
        "BPF": "able to use Berkeley Packet Filter programs",
        "CHECKPOINT_RESTORE": "able to use checkpoint/restore functionality",
        "CHOWN": "able to change file ownership arbitrarily",
        "DAC_OVERRIDE": "able to bypass file permission checks",
        "DAC_READ_SEARCH": "able to bypass read/execute permission checks",
        "FOWNER": "able to bypass permission checks for file operations",
        "FSETID": "able to preserve set-user-ID and set-group-ID bits",
        "IPC_LOCK": "able to lock memory segments in RAM",
        "IPC_OWNER": "able to bypass permission checks for IPC operations",
        "KILL": "able to send signals to any process",
        "LEASE": "able to establish file leases",
        "LINUX_IMMUTABLE": "able to set immutable and append-only file attributes",
        "MAC_ADMIN": "able to configure Mandatory Access Control",
        "MAC_OVERRIDE": "able to override Mandatory Access Control restrictions",
        "MKNOD": "able to create special files using mknod()",
        "NET_ADMIN": "able to perform network administration tasks",
        "NET_BIND_SERVICE": "able to bind to privileged ports (< 1024)",
        "NET_BROADCAST": "able to make socket broadcasts",
        "NET_RAW": "able to use raw and packet sockets",
        "PERFMON": "able to access performance monitoring interfaces",
        "SETFCAP": "able to set file capabilities on other files",
        "SETGID": "able to change group ID of processes",
        "SETPCAP": "able to transfer capabilities between processes",
        "SETUID": "able to change user ID of processes",
        "SYS_ADMIN": "able to perform system administration operations",
        "SYS_BOOT": "able to reboot and load/unload kernel modules",
        "SYS_CHROOT": "able to use chroot() system call",
        "SYS_MODULE": "able to load and unload kernel modules",
        "SYS_NICE": "able to modify process scheduling priority",
        "SYS_PACCT": "able to configure process accounting",
        "SYS_PTRACE": "able to trace and control other processes",
        "SYS_RAWIO": "able to perform raw I/O operations",
        "SYS_RESOURCE": "able to override resource limits",
        "SYS_TIME": "able to set system clock and real-time clock",
        "SYS_TTY_CONFIG": "able to configure TTY devices",
        "SYSLOG": "able to perform privileged syslog operations",
        "WAKE_ALARM": "able to trigger system wake alarms",
    }

    _RENAME_MAPPINGS = {
        "dssystem": "DS System",
        "npm": "Nginx Proxy Manager",
        "omada": "Omada Controller",
        "lms": "Lyrion Media Server",
    }

    @staticmethod
    def service_name_to_title(service_name: str) -> str:
        """Convert a service name to a human-readable title."""
        return service_name.replace("-", " ").replace("_", " ").title()

    @staticmethod
    def hash_service_name(service_name: str) -> str:
        """Hash a service name to a short, unique identifier."""
        return service_name.lower().replace("_", "").replace("-", "").replace(" ", "")

    @classmethod
    def create_capability_description(cls, capability_name: str, service_names: List[str], title: str) -> str:
        """Create a human-readable description for a capability and its services."""
        if capability_name not in cls._CAPABILITY_DESCRIPTIONS:
            raise ValueError(f"Unknown capability: {capability_name}")

        if not service_names:
            raise ValueError(f"No services provided for capability: {capability_name}")
        clean_service_names = set()
        for name in service_names:
            parts = name.split("-")
            if parts[-1].isnumeric():
                name = "-".join(parts[:-1])
            clean_service_names.add(name)

        formatted_services = []
        for name in clean_service_names:
            if cls.hash_service_name(name) == cls.hash_service_name(title):
                formatted_services.append(title)
            elif name.lower() in cls._RENAME_MAPPINGS:
                formatted_services.append(cls._RENAME_MAPPINGS[name.lower()])
            else:
                formatted_services.append(cls.service_name_to_title(name))

        base_description = cls._CAPABILITY_DESCRIPTIONS[capability_name]

        if len(formatted_services) == 1:
            return f"{formatted_services[0]} is {base_description}"
        else:
            return f"{', '.join(sorted(formatted_services))} are {base_description}"


class FileSystemCache:
    """Simple file system cache to avoid repeated file reads."""

    def __init__(self):
        self._yaml_cache = {}

    def read_yaml_file(self, file_path: Path) -> Dict:
        """Read and cache YAML file contents."""
        # Convert to string for hashing
        file_key = str(file_path)

        # Check if file is cached and still valid
        if file_key in self._yaml_cache:
            cached_data, cached_mtime = self._yaml_cache[file_key]
            try:
                current_mtime = file_path.stat().st_mtime
                if current_mtime == cached_mtime:
                    return cached_data
            except OSError:
                # File might have been deleted, remove from cache
                del self._yaml_cache[file_key]

        # Read and cache the file
        try:
            with open(file_path, "r") as f:
                data = yaml.safe_load(f)

            # Ensure we have a dict
            if not isinstance(data, dict):
                raise ValueError(f"YAML file {file_path} must contain a dictionary at root level, got {type(data)}")

            # Cache with modification time
            mtime = file_path.stat().st_mtime
            self._yaml_cache[file_key] = (data, mtime)
            return data

        except (IOError, yaml.YAMLError) as e:
            raise RuntimeError(f"Failed to read YAML file {file_path}") from e

    def write_yaml_file(self, file_path: Path, data: Dict) -> None:
        """Write YAML data to file and invalidate cache."""
        try:
            with open(file_path, "w") as f:
                yaml.dump(data, f, default_flow_style=False, sort_keys=False)

            # Remove from cache since file was modified
            file_key = str(file_path)
            if file_key in self._yaml_cache:
                del self._yaml_cache[file_key]

        except IOError as e:
            raise RuntimeError(f"Failed to write YAML file {file_path}") from e

    def clear_cache(self) -> None:
        """Clear all cached data."""
        self._yaml_cache.clear()


class AppDiscoveryService:
    """Service for discovering and validating TrueNAS apps."""

    def __init__(self, apps_root_dir: str = Config.APPS_ROOT_DIR):
        self.apps_root_path = Path(apps_root_dir)

    def discover_all_apps(self) -> List[AppManifest]:
        """Discover all valid apps across all trains."""
        if not self.apps_root_path.exists():
            logger.error(f"Apps root directory {self.apps_root_path} does not exist")
            return []

        all_apps = []
        for train_path in self.apps_root_path.iterdir():
            if not train_path.is_dir():
                continue

            train_name = train_path.name
            logger.info(f"Scanning train: {train_name}")

            train_apps = self._discover_apps_in_train(train_path, train_name)
            all_apps.extend(train_apps)

        logger.info(f"Discovered {len(all_apps)} apps total")
        return all_apps

    def discover_single_app(self, train_name: str, app_name: str) -> Optional[AppManifest]:
        """Discover a specific app by train and name."""
        app_path = self.apps_root_path / train_name / app_name
        return self._create_app_manifest(app_path, train_name)

    def _discover_apps_in_train(self, train_path: Path, train_name: str) -> List[AppManifest]:
        """Discover all apps within a specific train."""
        apps = []
        for app_path in train_path.iterdir():
            if not app_path.is_dir():
                continue

            app_manifest = self._create_app_manifest(app_path, train_name)
            if app_manifest:
                apps.append(app_manifest)

        return apps

    def _create_app_manifest(self, app_path: Path, train_name: str) -> Optional[AppManifest]:
        """Create an AppManifest for a single app directory."""
        app_name = app_path.name

        # Skip excluded apps in test train
        if train_name == "test" and app_name in Config.EXCLUDED_TEST_APPS:
            logger.debug(f"Skipping excluded test app: {app_name}")
            return None

        # Validate required files exist
        if not (app_path / Config.APP_METADATA_FILE).exists():
            logger.warning(f"Skipping {app_path}: missing {Config.APP_METADATA_FILE}")
            return None

        # Find test value files
        test_values_path = app_path / Config.TEST_VALUES_DIR
        test_value_files = []

        if test_values_path.exists():
            test_value_files = [f.name for f in test_values_path.iterdir() if f.is_file() and f.suffix == ".yaml"]

        if not test_value_files:
            logger.warning(f"No test values found for {app_path}")

        logger.debug(f"Found app: {app_path} with {len(test_value_files)} test configurations")
        return AppManifest(path=app_path, name=app_name, train=train_name, test_value_files=test_value_files)


class DockerComposeRenderer:
    """Handles rendering TrueNAS apps using Docker container."""

    def __init__(self, container_image: str = Config.CONTAINER_IMAGE, platform: str = Config.PLATFORM):
        self.container_image = container_image
        self.platform = platform

    def render_app_with_values(self, app_manifest: AppManifest, test_values_filename: str) -> Dict:
        """Render an app with specific test values and return compose data."""
        workspace_path = os.getcwd()
        values_path = app_manifest.path / Config.TEST_VALUES_DIR / test_values_filename

        docker_cmd = [
            "docker",
            "run",
            f"--platform={self.platform}",
            "--quiet",
            "--rm",
            f"-v={workspace_path}:/workspace",
            self.container_image,
            "apps_render_app",
            "render",
            f"--path=/workspace/{app_manifest.path}",
            f"--values=/workspace/{values_path}",
        ]

        logger.debug(f"Rendering: {' '.join(docker_cmd)}")

        try:
            result = subprocess.run(docker_cmd, capture_output=True, text=True, check=True)
            logger.debug(f"Successfully rendered {app_manifest.name} with {test_values_filename}")

            if result.stdout:
                logger.debug(f"Render output: {result.stdout}")

        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to render {app_manifest.name}/{test_values_filename}")
            logger.error(f"Docker error: {e.stderr}")
            raise RuntimeError(f"Rendering failed for {app_manifest.name}") from e

        # Read rendered compose file
        compose_path = app_manifest.path / Config.RENDERED_COMPOSE_PATH
        if not compose_path.exists():
            raise FileNotFoundError(f"Rendered compose file not found: {compose_path}")

        try:
            self._fix_file_permissions(compose_path)
            with open(compose_path, "r") as f:
                data = yaml.safe_load(f)

            # Ensure we have a dict
            if not isinstance(data, dict):
                raise ValueError(f"YAML file {compose_path} must contain a dictionary at root level, got {type(data)}")
            return data

        except yaml.YAMLError as e:
            raise RuntimeError(f"Failed to parse rendered compose: {compose_path}") from e

    def _fix_file_permissions(self, file_path: Path) -> None:
        """Fix file permissions using Docker container."""
        logger.debug(f"Fixing permissions for {file_path}")

        docker_cmd = [
            "docker",
            "run",
            f"--platform={self.platform}",
            "--quiet",
            "--rm",
            f"-v={os.getcwd()}:/workspace",
            "--entrypoint=/bin/bash",
            self.container_image,
            "-c",
            f"chmod 777 /workspace/{file_path}",
        ]

        result = subprocess.run(docker_cmd, capture_output=True, text=True)
        if result.returncode != 0:
            logger.error(f"Failed to fix permissions for {file_path}")
            logger.error(result.stderr)
            raise RuntimeError(f"Permission fix failed for {file_path}")


class DockerComposeAnalyzer:
    """Analyzes Docker Compose configurations for service capabilities."""

    @staticmethod
    def extract_service_names(compose_data: Dict, include_short_lived: bool = False) -> List[str]:
        """Extract service names from compose data, optionally including short-lived services."""
        services = compose_data.get("services", {})
        if not services:
            raise ValueError("No services found in compose data")

        service_names = []
        for service_name, service_config in services.items():
            if not include_short_lived:
                restart_policy = service_config.get("restart", "")
                if restart_policy.startswith("on-failure"):
                    logger.debug(f"Skipping short-lived service: {service_name}")
                    continue

            service_names.append(service_name)

        return service_names

    @staticmethod
    def extract_capabilities_by_service(compose_data: Dict) -> Dict[str, Set[str]]:
        """Extract capabilities grouped by service from compose data."""
        services = compose_data.get("services", {})
        if not services:
            raise ValueError("No services found in compose data")

        service_capabilities = {}

        for service_name, service_config in services.items():
            restart_policy = service_config.get("restart", "")

            # Skip services without restart policy
            if not restart_policy:
                logger.warning(f"No restart policy for service: {service_name}")
                continue

            # Skip short-lived services
            if restart_policy.startswith("on-failure"):
                logger.debug(f"Skipping short-lived service: {service_name}")
                continue

            # Validate cap_drop configuration
            if "cap_drop" not in service_config:
                logger.error(
                    f"No cap_drop for service: {service_name}. Consider explicitly setting the defaults via cap_add "
                    "https://github.com/moby/moby/blob/7a0bf747f5c25da0794e42d5f9e5a40db5a7786e/oci/caps/defaults.go#L4"
                )
            elif service_config["cap_drop"] != ["ALL"]:
                logger.error(f"Non-standard cap_drop for service: {service_name}")

            # Extract capabilities
            if "cap_add" in service_config:
                capabilities = set(service_config["cap_add"])
                if capabilities:
                    service_capabilities[service_name] = capabilities
                    logger.debug(f"Service {service_name} capabilities: {capabilities}")

        return service_capabilities


class AppQuestionsValidator:
    """Validates app questions configuration against actual services."""

    def __init__(self, file_cache: FileSystemCache):
        self.file_cache = file_cache

    def validate_container_labels_section(self, app_manifest: AppManifest, service_names: List[str]) -> None:
        """Validate that container labels section matches actual service names."""
        questions_path = app_manifest.path / Config.QUESTIONS_FILE
        if not questions_path.exists():
            raise FileNotFoundError(f"Questions file not found: {questions_path}")

        questions_config = self.file_cache.read_yaml_file(questions_path)
        if not isinstance(questions_config, dict):
            raise ValueError(f"Invalid questions config in {questions_path}")

        # Find the labels question and validate containers enum
        for question in questions_config.get("questions", []):
            if question.get("variable") != "labels":
                continue

            for item in question["schema"]["items"][0]["schema"]["attrs"]:
                if item.get("variable") != "containers":
                    continue

                old_enum = item["schema"]["items"][0]["schema"]["enum"]
                new_enum = [{"value": name, "description": name} for name in service_names]

                old_values = {item["value"] for item in old_enum}
                new_values = {item["value"] for item in new_enum}

                if old_values != new_values:
                    raise ValueError(
                        f"Container labels section should have {sorted(new_values)} " f"but has {sorted(old_values)}"
                    )
            break


class AppVersionManager:
    """Manages app version information and updates."""

    def __init__(self, file_cache: FileSystemCache, should_bump_versions: bool = True):
        self.file_cache = file_cache
        self.should_bump_versions = should_bump_versions

    def get_current_app_version(self, app_manifest: AppManifest) -> str:
        """Get the current app version from appropriate source."""
        # Special case for ix-app
        if str(app_manifest.path) == f"{Config.APPS_ROOT_DIR}/stable/ix-app":
            app_config = self.file_cache.read_yaml_file(app_manifest.path / Config.APP_METADATA_FILE)
            return app_config["version"]

        # Regular apps use ix_values.yaml
        values_path = app_manifest.path / Config.APP_VALUES_FILE
        if not values_path.exists():
            raise FileNotFoundError(f"App values file not found: {values_path}")

        values_config = self.file_cache.read_yaml_file(values_path)
        return values_config["images"]["image"]["tag"]

    def increment_patch_version(self, version: str) -> str:
        """Increment the patch version number."""
        if not self.should_bump_versions:
            return version

        try:
            parts = version.split(".")
            if len(parts) != 3:
                raise ValueError("Version must be in format x.y.z")

            parts[2] = str(int(parts[2]) + 1)
            return ".".join(parts)
        except (ValueError, IndexError) as e:
            raise ValueError(f"Invalid version format: {version}") from e


class AppMetadataUpdater:
    """Updates app metadata files with capability information."""

    def __init__(self, file_cache: FileSystemCache, version_manager: AppVersionManager):
        self.file_cache = file_cache
        self.version_manager = version_manager

    def update_app_metadata(
        self,
        app_manifest: AppManifest,
        capabilities: List[DockerCapability],
        current_app_version: str,
        should_bump_version: bool = True,
    ) -> None:
        """Update app.yaml with new capabilities and version information."""
        app_metadata_path = app_manifest.path / Config.APP_METADATA_FILE
        app_config = self.file_cache.read_yaml_file(app_metadata_path)

        if not isinstance(app_config, dict):
            raise ValueError(f"Invalid app config in {app_metadata_path}")

        # Validate app configuration (warnings only)
        self._validate_app_configuration(app_manifest, app_config)

        # Check if update is needed
        needs_version_bump = False

        # Update app version if changed
        old_app_version = app_config.get("app_version", "")
        # If the old app version is a substring of the current version, keep it
        # Example new version is 1.2.3-debian and app_version is 1.2.3. This is fine.
        if old_app_version in current_app_version:
            current_app_version = old_app_version
        if current_app_version != old_app_version:
            needs_version_bump = True
        app_config["app_version"] = current_app_version

        # Update capabilities if changed
        new_capabilities_data = sorted([cap.to_dict() for cap in capabilities], key=lambda c: c["name"])
        old_capabilities_data = sorted(app_config.get("capabilities", []), key=lambda c: c["name"])

        if old_capabilities_data != new_capabilities_data:
            needs_version_bump = True
        app_config["capabilities"] = new_capabilities_data

        # Bump version if needed
        if needs_version_bump and should_bump_version:
            old_version = app_config["version"]
            new_version = self.version_manager.increment_patch_version(old_version)
            app_config["version"] = new_version
            logger.info(f"Updated {app_manifest.name} version: {old_version} → {new_version}")

        # Write updated configuration
        self.file_cache.write_yaml_file(app_metadata_path, app_config)

    def _validate_app_configuration(self, app_manifest: AppManifest, app_config: Dict) -> None:
        """Validate app configuration and log warnings for issues."""
        app_name = app_config.get("name", app_manifest.name)

        # Check for multiple categories
        categories = app_config.get("categories", [])
        if len(categories) > 1:
            logger.warning(f"{app_manifest.name}: multiple categories: {categories}")

        # Check for missing fields
        if "date_added" not in app_config:
            logger.warning(f"{app_manifest.name}: missing date_added")
        if "changelog_url" not in app_config:
            logger.warning(f"{app_manifest.name}: missing changelog_url")

        # Validate media URLs
        expected_base_url = f"https://media.sys.truenas.net/apps/{app_name}"

        icon_url = app_config.get("icon", "")
        if not icon_url.startswith(f"{expected_base_url}/icons/"):
            logger.warning(f"{app_manifest.name}: invalid icon URL: {icon_url}")

        for screenshot_url in app_config.get("screenshots", []):
            if not screenshot_url.startswith(f"{expected_base_url}/screenshots/"):
                logger.warning(f"{app_manifest.name}: invalid screenshot URL: {screenshot_url}")


class TrueNASAppCapabilityManager:
    """Main orchestrator for TrueNAS app capability management."""

    def __init__(self, should_bump_versions: bool = True):
        self.should_bump_versions = should_bump_versions
        self.file_cache = FileSystemCache()
        self.discovery_service = AppDiscoveryService()
        self.compose_renderer = DockerComposeRenderer()
        self.compose_analyzer = DockerComposeAnalyzer()
        self.questions_validator = AppQuestionsValidator(self.file_cache)
        self.version_manager = AppVersionManager(self.file_cache, should_bump_versions)
        self.metadata_updater = AppMetadataUpdater(self.file_cache, self.version_manager)
        self.capability_registry = DockerCapabilityRegistry()

    def analyze_single_app(self, app_manifest: AppManifest) -> AppAnalysisResult:
        """Analyze a single app across all its test configurations."""
        if not app_manifest.test_value_files:
            logger.warning(f"No test configurations for {app_manifest.name}")
            return AppAnalysisResult([], [], "")

        # Extract app title from app.yaml
        app_metadata_path = app_manifest.path / Config.APP_METADATA_FILE
        app_config = self.file_cache.read_yaml_file(app_metadata_path)
        if not isinstance(app_config, dict):
            raise ValueError(f"Invalid app config in {app_metadata_path}")
        app_title = app_config.get("title", app_manifest.name)

        # Track capabilities across all test configurations
        capability_to_services: Dict[str, Set[str]] = {}
        all_service_names = set()

        for test_values_file in app_manifest.test_value_files:
            logger.debug(f"Processing {app_manifest.name} with {test_values_file}")

            try:
                # Render app with test configuration
                compose_data = self.compose_renderer.render_app_with_values(app_manifest, test_values_file)

                # Extract service information
                service_names = self.compose_analyzer.extract_service_names(compose_data)
                all_service_names.update(service_names)

                # Extract capabilities by service
                service_capabilities = self.compose_analyzer.extract_capabilities_by_service(compose_data)

                # Aggregate capabilities
                for service_name, capabilities in service_capabilities.items():
                    for capability in capabilities:
                        if capability not in capability_to_services:
                            capability_to_services[capability] = set()
                        capability_to_services[capability].add(service_name)

            except Exception as e:
                logger.error(f"Failed to process {app_manifest.name}/{test_values_file}: {e}")
                raise

        # Convert to DockerCapability objects
        capabilities = []
        for capability_name, services in capability_to_services.items():
            try:
                description = self.capability_registry.create_capability_description(
                    capability_name, sorted(services), app_title
                )
                capabilities.append(DockerCapability(capability_name, description))
            except ValueError as e:
                logger.error(f"Failed to create capability description: {e}")
                continue

        # Get current app version
        current_version = self.version_manager.get_current_app_version(app_manifest)

        return AppAnalysisResult(
            capabilities=sorted(capabilities, key=lambda c: c.name),
            service_names=sorted(all_service_names),
            app_version=str(current_version),
        )

    def update_single_app(self, app_manifest: AppManifest) -> None:
        """Update capabilities and metadata for a single app."""
        logger.debug(f"Updating capabilities for {app_manifest.name}")

        try:
            # Analyze the app
            analysis_result = self.analyze_single_app(app_manifest)

            # Validate questions configuration
            self.questions_validator.validate_container_labels_section(app_manifest, analysis_result.service_names)

            # Update metadata
            self.metadata_updater.update_app_metadata(
                app_manifest, analysis_result.capabilities, analysis_result.app_version, self.should_bump_versions
            )

            logger.info(
                f"Successfully updated {app_manifest.name} with " f"{len(analysis_result.capabilities)} capabilities"
            )

        except Exception as e:
            logger.error(f"Failed to update {app_manifest.name}: {e}")
            raise

    def update_all_apps(self) -> None:
        """Update capabilities for all discovered apps."""
        app_manifests = self.discovery_service.discover_all_apps()

        if not app_manifests:
            logger.warning("No apps found to process")
            return

        success_count = 0
        failed_count = 0

        for app_manifest in app_manifests:
            try:
                self.update_single_app(app_manifest)
                success_count += 1
            except Exception as e:
                logger.error(f"Skipping {app_manifest.name}: {e}")
                failed_count += 1
                continue

        logger.info(f"Successfully processed {success_count}/{len(app_manifests)} apps")
        if failed_count > 0:
            logger.error(f"Failed to process {failed_count} apps")
            sys.exit(1)

    def update_specific_app(self, train_name: str, app_name: str) -> None:
        """Update capabilities for a specific app."""
        app_manifest = self.discovery_service.discover_single_app(train_name, app_name)
        if not app_manifest:
            logger.error(f"App {train_name}/{app_name} not found")
            sys.exit(1)

        self.update_single_app(app_manifest)


def parse_command_line_arguments() -> argparse.Namespace:
    """Parse and return command line arguments."""
    parser = argparse.ArgumentParser(
        description="TrueNAS Apps Capability Manager - Analyze and update app capabilities"
    )
    parser.add_argument("--train", help="Specific train name to process")
    parser.add_argument("--app", help="Specific app name to process (requires --train)")
    parser.add_argument("--no-bump", action="store_true", help="Skip version bumping when updating metadata")

    return parser.parse_args()


def main():
    """Main application entry point."""
    try:
        args = parse_command_line_arguments()

        # Initialize the capability manager
        capability_manager = TrueNASAppCapabilityManager(should_bump_versions=not args.no_bump)

        # Determine operation mode
        if args.train and args.app:
            # Update specific app
            capability_manager.update_specific_app(args.train, args.app)
        elif args.train or args.app:
            # Invalid: both train and app must be provided together
            logger.error("Both --train and --app must be provided together, or neither")
            sys.exit(1)
        else:
            # Update all apps
            capability_manager.update_all_apps()

    except KeyboardInterrupt:
        logger.info("Operation cancelled by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
